{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf66ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdt_tokenizer.core import MyanmarSyllableTokenizer\n",
    "from mmdt_tokenizer.utils.config import DATA_DIR, OUTPUT_DIR\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "\n",
    "tokenizer = MyanmarSyllableTokenizer()\n",
    "\n",
    "def test_syllable_tokenize_basic(tokenizer:  MyanmarSyllableTokenizer):\n",
    "    text = \"အစတေးခံများ အထိမ်းအမှတ် သရေခေတ္တရာ\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    assert isinstance(tokens, list)\n",
    "\n",
    "def test_syllable_tokenize_multiple_text(tokenizer:  MyanmarSyllableTokenizer):\n",
    "    \"\"\"Tests the CSV saving feature exposed by the main tokenizer.\"\"\"\n",
    "    text = [\"မင်္ဂလာပါ မြန်မာစာ\", \"နေ လင့်ကစား လင့်ကစား တနေ့တော့\"]\n",
    "    csv_output_path = OUTPUT_DIR / \"result_syllable_test.csv\"\n",
    "    tokens = tokenizer.tokenize(text,save_csv=str(csv_output_path), conll_style=False)\n",
    "    assert(len(tokens)==2)\n",
    "    \n",
    "    \n",
    "def test_syllable_tokenize_csv(tokenizer: MyanmarSyllableTokenizer):\n",
    "    \"\"\"Tests the CSV loading/saving feature exposed by the main tokenizer.\"\"\"\n",
    "    csv_input_path = DATA_DIR / \"test_data.csv\"\n",
    "    csv_output_path = OUTPUT_DIR / \"result_syllable_bd.csv\"\n",
    "\n",
    "    df = pd.read_csv(csv_input_path)\n",
    "    \n",
    "    # Call the main tokenizer method with the save_csv argument and input is dataframe\n",
    "    \n",
    "    tokenizer.tokenize(df, column = 'original_sentence', save_csv=str(csv_output_path), conll_style=False)\n",
    "    assert Path(csv_output_path).exists()\n",
    "    \n",
    "\n",
    "test_syllable_tokenize_basic(tokenizer)\n",
    "test_syllable_tokenize_multiple_text(tokenizer)\n",
    "test_syllable_tokenize_csv(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3277bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk(span=(0, 0), text='ငွေ', tag='RAW')\n",
      "Chunk(span=(1, 1), text='၁၀၀,၀၀၀,၀၀', tag='NUM')\n",
      "Chunk(span=(2, 2), text='ကျပ်', tag='RAW')\n",
      "Chunk(span=(3, 3), text='၀၉၇၈၈၃၄၄၅၆ ', tag='NUM')\n",
      "Chunk(span=(4, 4), text='တစ်', tag='NUM')\n",
      "Chunk(span=(5, 5), text='ထောင့်', tag='NUM')\n",
      "Chunk(span=(6, 6), text='နှစ်', tag='NUM')\n",
      "Chunk(span=(7, 7), text='ရာ', tag='NUM')\n",
      "Chunk(span=(8, 8), text='နှစ်', tag='NUM')\n",
      "Chunk(span=(9, 9), text='ဆယ့်', tag='NUM')\n",
      "Chunk(span=(10, 10), text='လေး', tag='NUM')\n",
      "Chunk(span=(13, 13), text='၃', tag='NUM')\n",
      "Chunk(span=(15, 15), text='စစ်', tag='RAW')\n",
      "Chunk(span=(16, 16), text='တပ်', tag='RAW')\n",
      "Chunk(span=(17, 17), text='က', tag='POSTP')\n",
      "Chunk(span=(18, 18), text='သုံး', tag='NUM')\n",
      "Chunk(span=(19, 19), text='ခါ', tag='RAW')\n",
      "Chunk(span=(20, 20), text='တိုက်', tag='RAW')\n",
      "Chunk(span=(21, 21), text='တယ်', tag='SFP')\n",
      "end\n",
      "[['ငွေ၁၀၀,၀၀၀,၀၀ကျပ်၀၉၇၈၈၃၄၄၅၆ တစ်ထောင့်နှစ်ရာနှစ်ဆယ့်', 'လေးခုနှစ်', '၃ခု', 'စစ်တပ်', 'က', 'သုံးခါတိုက်တယ်']]\n"
     ]
    }
   ],
   "source": [
    "def test_postpositions(tokenizer):\n",
    "    text= \"စစ်ရေးအရ တကက-၁၃ ရှိတဲ့ တွင်းငယ်ရွာကြီးကို စစ်တပ်ထိန်းချုပ်လိုက်ပြီးနောက် ၇ မိုင်တပ်စခန်းအနီးက စမ္ပါယ်နဂိုရ်ရွာ၊ ကျောက်ကြီး၊ ဝါးဖြူတောင် စတဲ့နေရာတွေကိုလည်း အဆင့်ဆင့် လက်လွှတ်ခဲ့ရပြီး မြို့ကို ဆုံးရှုံးလိုက်ရတာလို့ မြေပြင်သတင်းရင်းမြစ်တစ်ဦးက ဆိုပါတယ်။\"\n",
    "    #text = \"ကချင်၊ စစ်ကိုင်း နဲ့ ရှမ်း မြောက်ပိုင်း ဒေသသုံးခုပေါင်းဆုံရာ ဗျူဟာမြောက်မြို့ဖြစ်တဲ့ အင်းတော်မြို့ကို တော်လှန်ရေးတပ်တွေ စစ်ဆင်ရေးပြုလုပ်ရာမှာ ABSDF စစ်ကြောင်း ၁ ရဲ့ စစ်ကြောင်းမှူးက ကွပ်ကဲခဲ့ပါတယ်။\"\n",
    "    #text = \"သူဟာ အမေရိကန်နိုင်ငံ ဝါရှင်တန်မြို့က ကွန်မြူနစ် ဝါဒရဲ့ အစတေးခံများအထိမ်းအမှတ် ဖောင်ဒေးရှင်းရဲ့ အကြီးတန်း သုတေသီ ဖြစ်ပြီး ဒီ မှတ်တမ်းမှတ်ရာတွေကို ရှာဖွေတွေ့ရှိခဲ့သူ ဖြစ်ပါတယ်။\"\n",
    "    tokens = tokenizer.word_tokenize(text)\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def test_word_tokenize_multiple_text(tokenizer):\n",
    "    text = [\"သုံးခု/၃ခု တ-က-က-၁၃ သမ္မတ  10- 10 -2025 လင့်ကစား\", \"၁၉၈၀..... 1980 ၁၉၅၀ခုနှစ်က ငွေ၁၀၀,၀၀၀,၀၀ကျပ် တစ် ထောင့်နှစ်ရာနှစ်ဆယ့်လေးခုနှစ် ၊ အသက်(၈၀) \"]\n",
    "    csv_output_path = OUTPUT_DIR / \"result_word_test.csv\"\n",
    "    tokens = tokenizer.word_tokenize(text,save_csv=str(csv_output_path), conll_style=False)\n",
    "    print(tokens)\n",
    "    \n",
    "    assert(len(tokens)==2)\n",
    "    return tokens\n",
    "\n",
    "def est_word_tokenize_protector_cases(tokenizer):\n",
    "    text = \"ငွေ၁၀၀,၀၀၀,၀၀ကျပ် ၀၉၇၈၈၃၄၄၅၆ တစ် ထောင့်နှစ်ရာနှစ်ဆယ့်လေးခုနှစ် ၃ခု စစ်တပ်က သုံးခါ တိုက်တယ်\"\n",
    "    tokens = tokenizer.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "from mmdt_tokenizer import MyanmarTokenizer\n",
    "tokenizer = MyanmarTokenizer()\n",
    "#all_tokens = test_postpositions(tokenizer)\n",
    "#all_tokens = test_word_tokenize_multiple_text(tokenizer)\n",
    "result = est_word_tokenize_protector_cases(tokenizer)\n",
    "print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
