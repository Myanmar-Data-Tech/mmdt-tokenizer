{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf66ec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['နေ', 'လ', 'င့်', 'က', 'စား']]\n",
      "[['မင်္ဂ', 'လာ', 'ပါ', 'မြန်', 'မာ', 'စာ'], ['အ', 'ကန့်', 'အ', 'သတ်', 'လင့်', 'က', 'စား', 'တ', 'နေ့', 'တော့']]\n"
     ]
    }
   ],
   "source": [
    "from mmdt_tokenizer.core import MyanmarSyllableTokenizer\n",
    "from mmdt_tokenizer.utils.config import DATA_DIR, OUTPUT_DIR\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "tokenizer = MyanmarSyllableTokenizer()\n",
    "\n",
    "def test_syllable_tokenize_basic(tokenizer:  MyanmarSyllableTokenizer):\n",
    "    text = \"မင်္ဂလာပါ\"\n",
    "    text = \"နေလင့်ကစား\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(tokens)\n",
    "    assert isinstance(tokens, list)\n",
    "    #assert any(\"မင်္\" in tok or \"ဂ\" in tok for tok in tokens[0])\n",
    "\n",
    "def test_syllable_tokenize_multiple_text(tokenizer:  MyanmarSyllableTokenizer):\n",
    "    \"\"\"Tests the CSV saving feature exposed by the main tokenizer.\"\"\"\n",
    "    text = [\"မင်္ဂလာပါ မြန်မာစာ\", \"အကန့် အသတ် လင့်ကစား တနေ့တော့\"]\n",
    "    csv_output_path = OUTPUT_DIR / \"result_syllable_test.csv\"\n",
    "    tokens = tokenizer.tokenize(text,save_csv=str(csv_output_path), conll_style=False)\n",
    "    print(tokens)\n",
    "    assert(len(tokens)==2)\n",
    "    \n",
    "    \n",
    "def test_syllable_tokenize_csv(tokenizer: MyanmarSyllableTokenizer):\n",
    "    \"\"\"Tests the CSV loading/saving feature exposed by the main tokenizer.\"\"\"\n",
    "    csv_input_path = DATA_DIR / \"test_data.csv\"\n",
    "    csv_output_path = OUTPUT_DIR / \"result_syllable_bd.csv\"\n",
    "\n",
    "    df = pd.read_csv(csv_input_path)\n",
    "    \n",
    "    # Call the main tokenizer method with the save_csv argument and input is dataframe\n",
    "    \n",
    "    tokenizer.tokenize(df, column = 'original_sentence', save_csv=str(csv_output_path), conll_style=False)\n",
    "    assert Path(csv_output_path).exists()\n",
    "    \n",
    "\n",
    "test_syllable_tokenize_basic(tokenizer)\n",
    "test_syllable_tokenize_multiple_text(tokenizer)\n",
    "test_syllable_tokenize_csv(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3277bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['သုံးခု', '၃ခု', 'တကက', '၁၃', 'သမ္', 'မတ', 'ကထိက', '10-10-2025', 'myo', '@', 'ဂျီမေလး', '.', 'ကွန်း'], ['၁၉၈၀', '1980', '၁၉၅၀', 'ခုနှစ်က', 'ငွေ', '၁၀၀,၀၀၀,၀၀', 'ကျပ်', 'တစ်', 'ထောင့်နှစ်ရာနှစ်ဆယ့်လေးခုနှစ်', '၊', 'အသက်', '၈၀']]\n"
     ]
    }
   ],
   "source": [
    "def test_postpositions(tokenizer):\n",
    "    text= \"စစ်ရေးအရ တကက-၁၃ ရှိတဲ့ တွင်းငယ်ရွာကြီးကို စစ်တပ်ထိန်းချုပ်လိုက်ပြီးနောက် ၇ မိုင်တပ်စခန်းအနီးက စမ္ပါယ်နဂိုရ်ရွာ၊ ကျောက်ကြီး၊ ဝါးဖြူတောင် စတဲ့နေရာတွေကိုလည်း အဆင့်ဆင့် လက်လွှတ်ခဲ့ရပြီး မြို့ကို ဆုံးရှုံးလိုက်ရတာလို့ မြေပြင်သတင်းရင်းမြစ်တစ်ဦးက ဆိုပါတယ်။\"\n",
    "    #text = \"ကချင်၊ စစ်ကိုင်း နဲ့ ရှမ်း မြောက်ပိုင်း ဒေသသုံးခုပေါင်းဆုံရာ ဗျူဟာမြောက်မြို့ဖြစ်တဲ့ အင်းတော်မြို့ကို တော်လှန်ရေးတပ်တွေ စစ်ဆင်ရေးပြုလုပ်ရာမှာ ABSDF စစ်ကြောင်း ၁ ရဲ့ စစ်ကြောင်းမှူးက ကွပ်ကဲခဲ့ပါတယ်။\"\n",
    "    text = \"သူဟာ အမေရိကန်နိုင်ငံ ဝါရှင်တန်မြို့က ကွန်မြူနစ် ဝါဒရဲ့ အစတေးခံများအထိမ်းအမှတ် ဖောင်ဒေးရှင်းရဲ့ အကြီးတန်း သုတေသီ ဖြစ်ပြီး ဒီ မှတ်တမ်းမှတ်ရာတွေကို ရှာဖွေတွေ့ရှိခဲ့သူ ဖြစ်ပါတယ်။\"\n",
    "    tokens = tokenizer.word_tokenize(text)\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def test_word_tokenize_multiple_text(tokenizer):\n",
    "    text = [\"သုံးခု/၃ခု တကက-၁၃ သမ္မတ ကထိက 10- 10 -2025 myo @ဂျီမေလး.ကွန်း\", \"၁၉၈၀..... 1980 ၁၉၅၀ခုနှစ်က ငွေ၁၀၀,၀၀၀,၀၀ကျပ် တစ် ထောင့်နှစ်ရာနှစ်ဆယ့်လေးခုနှစ် ၊ အသက်(၈၀) \"]\n",
    "    csv_output_path = OUTPUT_DIR / \"result_word_test.csv\"\n",
    "    tokens = tokenizer.word_tokenize(text,save_csv=str(csv_output_path), conll_style=False)\n",
    "    print(tokens)\n",
    "    \n",
    "    assert(len(tokens)==2)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "from mmdt_tokenizer import MyanmarTokenizer\n",
    "tokenizer = MyanmarTokenizer()\n",
    "#all_tokens = test_postpositions(tokenizer)\n",
    "all_tokens = test_word_tokenize_multiple_text(tokenizer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
