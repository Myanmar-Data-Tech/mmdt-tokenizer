{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf66ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdt_tokenizer.core import MyanmarSyllableTokenizer\n",
    "from mmdt_tokenizer.utils.config import DATA_DIR, OUTPUT_DIR\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "\n",
    "tokenizer = MyanmarSyllableTokenizer()\n",
    "\n",
    "def test_syllable_tokenize_basic(tokenizer:  MyanmarSyllableTokenizer):\n",
    "    text = \"အစတေးခံများ အထိမ်းအမှတ် သရေခေတ္တရာ\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    assert isinstance(tokens, list)\n",
    "\n",
    "def test_syllable_tokenize_multiple_text(tokenizer:  MyanmarSyllableTokenizer):\n",
    "    \"\"\"Tests the CSV saving feature exposed by the main tokenizer.\"\"\"\n",
    "    text = [\"မင်္ဂလာပါ မြန်မာစာ\", \"နေ လင့်ကစား လင့်ကစား တနေ့တော့\"]\n",
    "    csv_output_path = OUTPUT_DIR / \"result_syllable_test.csv\"\n",
    "    tokens = tokenizer.tokenize(text,save_csv=str(csv_output_path), conll_style=False)\n",
    "    assert(len(tokens)==2)\n",
    "    \n",
    "    \n",
    "def test_syllable_tokenize_csv(tokenizer: MyanmarSyllableTokenizer):\n",
    "    \"\"\"Tests the CSV loading/saving feature exposed by the main tokenizer.\"\"\"\n",
    "    csv_input_path = DATA_DIR / \"test_data.csv\"\n",
    "    csv_output_path = OUTPUT_DIR / \"result_syllable_bd.csv\"\n",
    "\n",
    "    df = pd.read_csv(csv_input_path)\n",
    "    \n",
    "    # Call the main tokenizer method with the save_csv argument and input is dataframe\n",
    "    \n",
    "    tokenizer.tokenize(df, column = 'original_sentence', save_csv=str(csv_output_path), conll_style=False)\n",
    "    assert Path(csv_output_path).exists()\n",
    "    \n",
    "\n",
    "test_syllable_tokenize_basic(tokenizer)\n",
    "test_syllable_tokenize_multiple_text(tokenizer)\n",
    "test_syllable_tokenize_csv(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3277bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ကချင်', 'စစ်ကိုင်း', 'နဲ့', 'ရှမ်း', 'မြောက်ပိုင်း', 'ဒေသ', 'သုံးခု', 'ပေါင်းဆုံ', 'ရာ', 'ဗျူဟာ', 'မြောက်မြို့', 'ဖြစ်', 'တဲ့', 'အင်းတော်မြို့', 'ကို', 'တော်လှန်ရေး', 'တပ်', 'တွေစစ်ဆင်ရေးပြုလုပ်', 'ရာ', 'မှာ', 'ABSDF', 'စစ်ကြောင်း', '၁', 'ရဲ့စစ်ကြောင်းမှူး', 'က', 'ကွပ်ကဲခဲ့ပါတယ်']]\n"
     ]
    }
   ],
   "source": [
    "def test_postpositions(tokenizer):\n",
    "    text= \"တပို့တွဲလတွင် ကသာမြို့ကို စစ်ရေးအရ တကက-၁၃ ရှိတဲ့ တွင်းငယ်ရွာကြီးကို စစ်တပ်ထိန်းချုပ်လိုက်ပြီးနောက် ၇ မိုင်တပ်စခန်းအနီးက စမ္ပါယ်နဂိုရ်ရွာ၊ ကျောက်ကြီး၊ ဝါးဖြူတောင် စတဲ့နေရာတွေကိုလည်း အဆင့်ဆင့် လက်လွှတ်ခဲ့ရပြီး မြို့ကို ဆုံးရှုံးလိုက်ရတာလို့ မြေပြင်သတင်းရင်းမြစ်တစ်ဦးက ဆိုပါတယ်။\"\n",
    "    text = \"ကချင်၊ စစ်ကိုင်း နဲ့ ရှမ်း မြောက်ပိုင်း ဒေသသုံးခုပေါင်းဆုံရာ ဗျူဟာမြောက်မြို့ဖြစ်တဲ့ အင်းတော်မြို့ကို တော်လှန်ရေးတပ်တွေ စစ်ဆင်ရေးပြုလုပ်ရာမှာ ABSDF စစ်ကြောင်း ၁ ရဲ့ စစ်ကြောင်းမှူးက ကွပ်ကဲခဲ့ ပါတယ်။\"\n",
    "    #text = \"၂၀၂၅ ခုနှစ်၊ မေလ ၂၇ ရက်မှာတော့ ABSDF နဲ့ SAF တို့က ကသာမြို့ကို သိမ်းပိုက်လိုက်ကြပါတယ်။\"\n",
    "    tokens = tokenizer.word_tokenize(text)\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def test_word_tokenize_multiple_text(tokenizer):\n",
    "    text = [\"သုံးခု/၃ခု တ-က-က-၁၃ သမ္မတ  10- 10 -2025 9:50, myothida@gmail.com လင့်ကစား\", \"၁၉၅၀ခုနှစ်က ငွေ၁၀၀,၀၀၀,၀၀ကျပ် တစ် ထောင့်နှစ်ရာနှစ်ဆယ့်လေးခုနှစ်၊ အသက်(၈၀) \"]\n",
    "    csv_output_path = OUTPUT_DIR / \"result_word_test.csv\"\n",
    "    tokens = tokenizer.word_tokenize(text,save_csv=str(csv_output_path), conll_style=False)\n",
    "    print(tokens)\n",
    "    assert(len(tokens)==2)\n",
    "    return tokens\n",
    "\n",
    "def test_word_tokenize_csv(tokenizer):\n",
    "    \"\"\"Tests the CSV loading/saving feature exposed by the main tokenizer.\"\"\"\n",
    "    csv_input_path = DATA_DIR / \"test_data.csv\"\n",
    "    csv_output_path = OUTPUT_DIR / \"result_word_bd.csv\"\n",
    "\n",
    "    df = pd.read_csv(csv_input_path)\n",
    "    tokenizer.word_tokenize(df, column = 'original_sentence', save_csv=str(csv_output_path), conll_style=False)\n",
    "    assert Path(csv_output_path).exists()\n",
    "\n",
    "\n",
    "from mmdt_tokenizer import MyanmarTokenizer\n",
    "tokenizer = MyanmarTokenizer()\n",
    "all_tokens = test_postpositions(tokenizer)\n",
    "#all_tokens = test_word_tokenize_multiple_text(tokenizer)\n",
    "#test_word_tokenize_csv(tokenizer)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
